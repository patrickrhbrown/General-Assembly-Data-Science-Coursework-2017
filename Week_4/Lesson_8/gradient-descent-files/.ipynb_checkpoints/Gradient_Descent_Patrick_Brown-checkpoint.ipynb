{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "We will be implementing logistic regression using the gradient descent algorithm.\n",
    "\n",
    "References:\n",
    "\n",
    "* [Andrew Ng's Machine Learning Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on Implementing Gradient Descent:\n",
    "\n",
    "* Implementing gradient descent can lead to challenging debugging. Try computing values by hand for a really simple example (1 feature, 2 data points) and make sure that your methods are getting the same values.\n",
    "* Numpy is your friend. Use the power of it! There should only be one\n",
    "loop in your code (in run). You should never have to loop over a\n",
    "numpy array. See the numpy tutorial anddocumentation ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Data\n",
    "\n",
    "* Generate a dataset using sklearn's make_classification module.\n",
    "* We use two features so that we can visualize our data. Make a scatterplot with the first feature on the x axis and the second on the y axis. Differentiate the positive results from the negative results somehow (different colors or different symbols).\n",
    "* Just by eye-balling, make an estimate for a good coefficient for this equation: y = mx of the decision boundary. We are using this form since our first implementation will not find an intercept. But also make an estimate for good coefficients for this equation: `y = mx+b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y = make_classification(n_samples = 100, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119f0dd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(X,Y,\"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for this model is: 0.92 out of 1\n"
     ]
    }
   ],
   "source": [
    "lm = LogisticRegression() #calling the linear regression object\n",
    "\n",
    "Y = Y\n",
    "X = X\n",
    "\n",
    "model = lm.fit(X,Y)\n",
    "\n",
    "score = model.score(X,Y)\n",
    "\n",
    "print \"The score for this model is: \" + str(score) + \" out of 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a10deafcf953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, coeffs):\n",
    "    \"\"\"Calculate the predicted conditional probabilities (floats between 0 and\n",
    "    1) for the given data with the given coefficients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: A 2 dimensional numpy array.  The data (independent variables) to use\n",
    "        for prediction.\n",
    "    coeffs: A 1 dimensional numpy array, the hypothesised coefficients.  Note\n",
    "        that the shape of X and coeffs must align.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predicted_probabilities: The conditional probabilities from the logistic\n",
    "        hypothesis function given the data and coefficients.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1.0/(coeffs+np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, coeffs, thresh=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the predicted class values (0 or 1) for the given data with the\n",
    "    given coefficients by comparing the predicted probabilities to a given\n",
    "    threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: A 2 dimensional numpy array.  The data (independent variables) to use\n",
    "        for prediction.\n",
    "    coeffs: A 1 dimensional numpy array, the hypothesized coefficients.  Note\n",
    "        that the shape of X and coeffs must align.\n",
    "    thresh: Threshold for comparison of probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predicted_class: The predicted class.\n",
    "    \"\"\"\n",
    "    z = np.dot(coeffs)\n",
    "    return predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(X, y, coeffs):\n",
    "    \"\"\"\n",
    "    Calculate the logistic cost function of the data with the given\n",
    "    coefficients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: A 2 dimensional numpy array.  The data (independent variables) to use\n",
    "        for prediction.\n",
    "    y: A 1 dimensional numpy array.  The actual class values of the response.\n",
    "        Must be encoded as 0's and 1's.  Also, must align properly with X and\n",
    "        coeffs.\n",
    "    coeffs: A 1 dimensional numpy array, the hypothesized coefficients.  Note\n",
    "        that the shape of X, y, and coeffs must align.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logistic_cost: The computed logistic cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    observations = len(y)\n",
    "    \n",
    "    predictions = predict(X,y)\n",
    "    \n",
    "    Class1_cost = (1-labels) * np.log(predictions)\n",
    "    \n",
    "    Class2_cost = (1-labels) * np.log(1-predictions)\n",
    "    \n",
    "    cost = class1_cost - class2_cost\n",
    "    \n",
    "    cost = cost.sum()/observations\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    \"\"\"Preform the gradient descent optimization algorithm for an arbitrary\n",
    "    cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost, gradient, predict_func, \n",
    "                 alpha=0.01,\n",
    "                 num_iterations=10000):\n",
    "        \"\"\"Initialize the instance attributes of a GradientDescent object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cost: The cost function to be minimized.\n",
    "        gradient: The gradient of the cost function.\n",
    "        predict_func: A function to make predictions after the optimization has\n",
    "            converged.\n",
    "        alpha: The learning rate.\n",
    "        num_iterations: Number of iterations to use in the descent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: The initialized GradientDescent object.\n",
    "        \"\"\"\n",
    "        \n",
    "        N = len(features)\n",
    "        \n",
    "        predictions = predict_proba(X, coeffs)\n",
    "        \n",
    "        gradient = np.dot(X.T, predictions - y)\n",
    "        \n",
    "        gradient /= N\n",
    "        \n",
    "        gradient *= lr\n",
    "        \n",
    "        coeffs -= gradient\n",
    "        \n",
    "        return coeffs\n",
    "        \n",
    "        # Initialize coefficients in run method once you know how many features\n",
    "        # you have.\n",
    "        self.coeffs = None\n",
    "        self.cost = cost\n",
    "        self.gradient = gradient\n",
    "        self.predict_func = predict_func\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "        \"\"\"Run the gradient descent algorithm for num_iterations repetitions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: A two dimensional numpy array.  The training data for the\n",
    "            optimization.\n",
    "        y: A one dimensional numpy array.  The training response for the\n",
    "            optimization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self:  The fit GradientDescent object.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        \"\"\"Call self.predict_func to return predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: Data to make predictions on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds: A one dimensional numpy array of predictions.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
