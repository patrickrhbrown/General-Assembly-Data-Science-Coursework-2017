{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickbrown/anaconda/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "# $matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/patrickbrown/Documents/GitHub/General-Assembly-Data-Science-Coursework-2017/Week_3/Lesson_8\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"logistic_regression_ex/data/grad.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317500</td>\n",
       "      <td>587.700000</td>\n",
       "      <td>3.389900</td>\n",
       "      <td>2.48500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466087</td>\n",
       "      <td>115.516536</td>\n",
       "      <td>0.380567</td>\n",
       "      <td>0.94446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.395000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre         gpa       rank\n",
       "count  400.000000  400.000000  400.000000  400.00000\n",
       "mean     0.317500  587.700000    3.389900    2.48500\n",
       "std      0.466087  115.516536    0.380567    0.94446\n",
       "min      0.000000  220.000000    2.260000    1.00000\n",
       "25%      0.000000  520.000000    3.130000    2.00000\n",
       "50%      0.000000  580.000000    3.395000    2.00000\n",
       "75%      1.000000  660.000000    3.670000    3.00000\n",
       "max      1.000000  800.000000    4.000000    4.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 4 columns):\n",
      "admit    400 non-null int64\n",
      "gre      400 non-null int64\n",
      "gpa      400 non-null float64\n",
      "rank     400 non-null int64\n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 12.6 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rank</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>97</td>\n",
       "      <td>93</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rank    1   2   3   4\n",
       "admit                \n",
       "0      28  97  93  55\n",
       "1      33  54  28  12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['admit'],data['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1197d0e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='rank', y='admit', data=data, saturation=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Logistic Regression with Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574302\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  admit   No. Observations:                  400\n",
      "Model:                          Logit   Df Residuals:                      396\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sat, 22 Jul 2017   Pseudo R-squ.:                 0.08107\n",
      "Time:                        12:06:25   Log-Likelihood:                -229.72\n",
      "converged:                       True   LL-Null:                       -249.99\n",
      "                                        LLR p-value:                 8.207e-09\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "gre            0.0023      0.001      2.101      0.036       0.000       0.004\n",
      "gpa            0.7770      0.327      2.373      0.018       0.135       1.419\n",
      "rank          -0.5600      0.127     -4.405      0.000      -0.809      -0.311\n",
      "intercept     -3.4495      1.133     -3.045      0.002      -5.670      -1.229\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# we set the intercept by declaring and adding a column to the dataframe\n",
    "\n",
    "data['intercept'] = 1.0\n",
    "\n",
    "# we split the target from the predictor columns by index slicing\n",
    "\n",
    "train_cols = data.columns[1:]\n",
    "\n",
    "# we call the logistic regression function on 'admit', passing predictors\n",
    "\n",
    "logit = sm.Logit(data['admit'], data[train_cols])\n",
    "\n",
    "# fit the model\n",
    "\n",
    "result = logit.fit()\n",
    "\n",
    "# print the result\n",
    "\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to recall that rank is reverse scored and so the negative correlation coefficient shows that the higher the rank of the undergraduate school, the greater the likelihood of the student's admission.  GPA also shows a strong correlation. GRE relatively weak correlation.\n",
    "\n",
    "Our P>|z| for rank is less than 0.0001 which means that it is exceedingly unlikely that we would see the same result if the null hypothesis (that there is no correlation between admit and rank) were true.  Equivalent markers for GRE and GPA give us less confidence to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1\n",
      "gre  -0.000561  0.003515\n",
      "gpa  -0.398366  0.390032\n",
      "rank -0.906985 -0.432091\n"
     ]
    }
   ],
   "source": [
    "print result.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import grid_search, cross_validation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank  intercept\n",
       "0      0  380  3.61     3        1.0\n",
       "1      1  660  3.67     3        1.0\n",
       "2      1  800  4.00     1        1.0\n",
       "3      1  640  3.19     4        1.0\n",
       "4      0  520  2.93     4        1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of gre vs admit is: 358309.3175\n",
      "The mean squared error of gpa vs admit is: 9.737741\n",
      "The mean squared error of rank vs admit is: 6.0175\n"
     ]
    }
   ],
   "source": [
    "x_pred_gre = data['gre']\n",
    "x_pred_gpa = data['gpa']\n",
    "x_pred_rank = data['rank']\n",
    "y_pred = data['admit']\n",
    "\n",
    "gre_mse = mean_squared_error(x_pred_gre,y_pred)\n",
    "gpa_mse = mean_squared_error(x_pred_gpa, y_pred)\n",
    "rank_mse = mean_squared_error(x_pred_rank, y_pred)\n",
    "\n",
    "print \"The mean squared error of gre vs admit is: \"  + str(gre_mse)\n",
    "print \"The mean squared error of gpa vs admit is: \" + str(gpa_mse)\n",
    "print \"The mean squared error of rank vs admit is: \" + str(rank_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression with SKLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error for gre is very large, with the mean squared error vs gpa and rank more modest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's run the model using all the predictors and see what scores we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for this model is: 0.7075 out of 1\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LogisticRegression() #calling the linear regression object\n",
    "\n",
    "Y = data['admit']\n",
    "X = data[train_cols]\n",
    "\n",
    "model = lm.fit(X,Y)\n",
    "\n",
    "score = model.score(X,Y)\n",
    "\n",
    "print \"The score for this model is: \" + str(score) + \" out of 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay about 70 per cent.  Can we obtain more insights from examining the coefficients? Let's put them in a dataframe and look further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gre</td>\n",
       "      <td>[0.00203746512577]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpa</td>\n",
       "      <td>[0.437413626296]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rank</td>\n",
       "      <td>[-0.586441708557]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intercept</td>\n",
       "      <td>[-1.02764986864]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                   1\n",
       "0        gre  [0.00203746512577]\n",
       "1        gpa    [0.437413626296]\n",
       "2       rank   [-0.586441708557]\n",
       "3  intercept    [-1.02764986864]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that gpa has a positive correlation to admission.  Rank is negatively correlated to admission. GRE appears to be adding little to the model in terms of prediction.\n",
    "\n",
    "So far we have tested and trained on the same dataset.  Let's split the data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.3, random_state = 0)\n",
    "model2 = linear_model.LogisticRegression()\n",
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "\n",
    "predicted = model2.predict(X_test)\n",
    "print predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62954913  0.37045087]\n",
      " [ 0.82490922  0.17509078]\n",
      " [ 0.82874059  0.17125941]\n",
      " [ 0.78189432  0.21810568]\n",
      " [ 0.61132421  0.38867579]\n",
      " [ 0.64341383  0.35658617]\n",
      " [ 0.89434468  0.10565532]\n",
      " [ 0.68858834  0.31141166]\n",
      " [ 0.84600212  0.15399788]\n",
      " [ 0.60707714  0.39292286]\n",
      " [ 0.81800466  0.18199534]\n",
      " [ 0.68135439  0.31864561]\n",
      " [ 0.47878778  0.52121222]\n",
      " [ 0.67661     0.32339   ]\n",
      " [ 0.35092802  0.64907198]\n",
      " [ 0.8941086   0.1058914 ]\n",
      " [ 0.34453694  0.65546306]\n",
      " [ 0.55511895  0.44488105]\n",
      " [ 0.40598091  0.59401909]\n",
      " [ 0.84734427  0.15265573]\n",
      " [ 0.72214392  0.27785608]\n",
      " [ 0.59802321  0.40197679]\n",
      " [ 0.69804173  0.30195827]\n",
      " [ 0.78485924  0.21514076]\n",
      " [ 0.62295387  0.37704613]\n",
      " [ 0.86208344  0.13791656]\n",
      " [ 0.50615146  0.49384854]\n",
      " [ 0.78491914  0.21508086]\n",
      " [ 0.42669489  0.57330511]\n",
      " [ 0.89363384  0.10636616]\n",
      " [ 0.79352172  0.20647828]\n",
      " [ 0.53894584  0.46105416]\n",
      " [ 0.77256771  0.22743229]\n",
      " [ 0.73384218  0.26615782]\n",
      " [ 0.60375383  0.39624617]\n",
      " [ 0.77686418  0.22313582]\n",
      " [ 0.55097553  0.44902447]\n",
      " [ 0.5707416   0.4292584 ]\n",
      " [ 0.84335044  0.15664956]\n",
      " [ 0.72304616  0.27695384]\n",
      " [ 0.82868845  0.17131155]\n",
      " [ 0.89571833  0.10428167]\n",
      " [ 0.51711414  0.48288586]\n",
      " [ 0.73076483  0.26923517]\n",
      " [ 0.69202017  0.30797983]\n",
      " [ 0.75979115  0.24020885]\n",
      " [ 0.59269399  0.40730601]\n",
      " [ 0.76277291  0.23722709]\n",
      " [ 0.59973677  0.40026323]\n",
      " [ 0.84745364  0.15254636]\n",
      " [ 0.68659611  0.31340389]\n",
      " [ 0.86824149  0.13175851]\n",
      " [ 0.58711789  0.41288211]\n",
      " [ 0.82106462  0.17893538]\n",
      " [ 0.54876781  0.45123219]\n",
      " [ 0.86377091  0.13622909]\n",
      " [ 0.84954688  0.15045312]\n",
      " [ 0.79831107  0.20168893]\n",
      " [ 0.42878832  0.57121168]\n",
      " [ 0.78767615  0.21232385]\n",
      " [ 0.84748352  0.15251648]\n",
      " [ 0.57440671  0.42559329]\n",
      " [ 0.89720829  0.10279171]\n",
      " [ 0.59776431  0.40223569]\n",
      " [ 0.71330212  0.28669788]\n",
      " [ 0.71823563  0.28176437]\n",
      " [ 0.80288567  0.19711433]\n",
      " [ 0.62863559  0.37136441]\n",
      " [ 0.73717207  0.26282793]\n",
      " [ 0.72125644  0.27874356]\n",
      " [ 0.70434567  0.29565433]\n",
      " [ 0.63634565  0.36365435]\n",
      " [ 0.43001144  0.56998856]\n",
      " [ 0.80198178  0.19801822]\n",
      " [ 0.68150842  0.31849158]\n",
      " [ 0.36544045  0.63455955]\n",
      " [ 0.65430202  0.34569798]\n",
      " [ 0.86424694  0.13575306]\n",
      " [ 0.46332344  0.53667656]\n",
      " [ 0.89913301  0.10086699]\n",
      " [ 0.8001059   0.1998941 ]\n",
      " [ 0.71700582  0.28299418]\n",
      " [ 0.68163558  0.31836442]\n",
      " [ 0.41391845  0.58608155]\n",
      " [ 0.62563142  0.37436858]\n",
      " [ 0.59235135  0.40764865]\n",
      " [ 0.82143103  0.17856897]\n",
      " [ 0.4562365   0.5437635 ]\n",
      " [ 0.55229797  0.44770203]\n",
      " [ 0.38121115  0.61878885]\n",
      " [ 0.57152677  0.42847323]\n",
      " [ 0.86506723  0.13493277]\n",
      " [ 0.79023088  0.20976912]\n",
      " [ 0.77809825  0.22190175]\n",
      " [ 0.6638415   0.3361585 ]\n",
      " [ 0.65373723  0.34626277]\n",
      " [ 0.77087135  0.22912865]\n",
      " [ 0.6132732   0.3867268 ]\n",
      " [ 0.73868878  0.26131122]\n",
      " [ 0.47309322  0.52690678]\n",
      " [ 0.86330956  0.13669044]\n",
      " [ 0.70971319  0.29028681]\n",
      " [ 0.72154418  0.27845582]\n",
      " [ 0.70824141  0.29175859]\n",
      " [ 0.88970688  0.11029312]\n",
      " [ 0.88579713  0.11420287]\n",
      " [ 0.65502956  0.34497044]\n",
      " [ 0.67770407  0.32229593]\n",
      " [ 0.76495998  0.23504002]\n",
      " [ 0.79258598  0.20741402]\n",
      " [ 0.85404901  0.14595099]\n",
      " [ 0.54186923  0.45813077]\n",
      " [ 0.49625339  0.50374661]\n",
      " [ 0.58405626  0.41594374]\n",
      " [ 0.8321055   0.1678945 ]\n",
      " [ 0.58347983  0.41652017]\n",
      " [ 0.56881602  0.43118398]\n",
      " [ 0.80541385  0.19458615]\n",
      " [ 0.90203904  0.09796096]\n",
      " [ 0.66551044  0.33448956]]\n"
     ]
    }
   ],
   "source": [
    "# predict class probabilities\n",
    "\n",
    "probability = model2.predict_proba(X_test)\n",
    "print probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above returns the same score we calculated earlier for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76  6]\n",
      " [30  8]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.93      0.81        82\n",
      "          1       0.57      0.21      0.31        38\n",
      "\n",
      "avg / total       0.67      0.70      0.65       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.confusion_matrix(y_test, predicted)\n",
    "print metrics.classification_report(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for this test data suggests that the model gave rise to only 6/120 false positives, but 30/120 false negatives.  The model correctly identified 76 true negatives and but only 8 true positives.\n",
    "\n",
    "Looking to the classification report the model has an average precision of 67% and a recall of 70%.  It appears to be more adept at identifying true negatives than true positives.\n",
    "\n",
    "The problem here is that the structure of the data would mean that given the number of admissions, we might do almost as well by building a model that simply chooses 'not admitted' 100% of the time.  To whit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a 31 per cent admission rate in the test sample, so a random guess would deliver about 69 per cent accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31666666666666665"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"There was a 31 per cent admission rate in the test sample, so a random guess would deliver about 69 per cent accuracy\"\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try to improve things further using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Evaluation Using Cross-Validation\n",
    "\n",
    "Let's try a 5 fold technique cross-validation to see if we can subject the accuracy to further interrogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71604938  0.74074074  0.7125      0.67088608  0.73417722]\n",
      "0.714870682919\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(linear_model.LogisticRegression(), X, Y, scoring='accuracy', cv=5)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, still at 71 per cent accuracy in most folds. To improve things, we're going to have to perhaps categorise the ranks column and see if that improves our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Dummified Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank_1  rank_2  rank_3  rank_4\n",
      "0       0       0       1       0\n",
      "1       0       0       1       0\n",
      "2       1       0       0       0\n",
      "3       0       0       0       1\n",
      "4       0       0       0       1\n"
     ]
    }
   ],
   "source": [
    "dummy_ranks = pd.get_dummies(data['rank'], prefix='rank')\n",
    "print dummy_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_cols = ['admit','gre','gpa']\n",
    "data = data[retain_cols].join(dummy_ranks.ix[:, 'rank_2':])\n",
    "data['intercept'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank_2  rank_3  rank_4  intercept\n",
       "0      0  380  3.61       0       1       0        1.0\n",
       "1      1  660  3.67       0       1       0        1.0\n",
       "2      1  800  4.00       0       0       0        1.0\n",
       "3      1  640  3.19       0       0       1        1.0\n",
       "4      0  520  2.93       0       0       1        1.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573147\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  admit   No. Observations:                  400\n",
      "Model:                          Logit   Df Residuals:                      394\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Sat, 22 Jul 2017   Pseudo R-squ.:                 0.08292\n",
      "Time:                        14:37:45   Log-Likelihood:                -229.26\n",
      "converged:                       True   LL-Null:                       -249.99\n",
      "                                        LLR p-value:                 7.578e-08\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "gre            0.0023      0.001      2.070      0.038       0.000       0.004\n",
      "gpa            0.8040      0.332      2.423      0.015       0.154       1.454\n",
      "rank_2        -0.6754      0.316     -2.134      0.033      -1.296      -0.055\n",
      "rank_3        -1.3402      0.345     -3.881      0.000      -2.017      -0.663\n",
      "rank_4        -1.5515      0.418     -3.713      0.000      -2.370      -0.733\n",
      "intercept     -3.9900      1.140     -3.500      0.000      -6.224      -1.756\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "train_cols = data.columns[1:]\n",
    "\n",
    "logit2 = sm.Logit(data['admit'], data[train_cols])\n",
    "\n",
    "# fit the model\n",
    "\n",
    "result2 = logit2.fit()\n",
    "\n",
    "# print the result\n",
    "\n",
    "print result2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see more clearly here that the probability of being granted entry is greater for students that attended a top ranked university at undergraduate level compared with a lower ranked graduate college.  The coefficient for gre remains of lower significance, and gpa shows strong correlation.  Let's run the model again in Sklearn as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at cross-validation and other scoring in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for this model is: 0.7075 out of 1\n"
     ]
    }
   ],
   "source": [
    "lm2 = linear_model.LogisticRegression() #calling the linear regression object\n",
    "\n",
    "Y2 = data['admit']\n",
    "X2 = data[train_cols]\n",
    "\n",
    "model2 = lm2.fit(X2,Y2)\n",
    "\n",
    "score2 = model2.score(X2,Y2)\n",
    "\n",
    "print \"The score for this model is: \" + str(score2) + \" out of 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No explicit improvement in the model score.  To be expected as we have not added anything, but we have removed a potential source of multicollinearity in the rank column by dropping the top rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gre</td>\n",
       "      <td>[0.00203746512577]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpa</td>\n",
       "      <td>[0.437413626296]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rank_2</td>\n",
       "      <td>[-0.586441708557]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rank_3</td>\n",
       "      <td>[-1.02764986864]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                   1\n",
       "0     gre  [0.00203746512577]\n",
       "1     gpa    [0.437413626296]\n",
       "2  rank_2   [-0.586441708557]\n",
       "3  rank_3    [-1.02764986864]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(X2.columns, np.transpose(model.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,Y2,test_size=0.3, random_state = 0)\n",
    "model3 = linear_model.LogisticRegression()\n",
    "model3.fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "\n",
    "predicted = model3.predict(X2_test)\n",
    "print predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63520492  0.36479508]\n",
      " [ 0.81506959  0.18493041]\n",
      " [ 0.8162539   0.1837461 ]\n",
      " [ 0.77449557  0.22550443]\n",
      " [ 0.61764144  0.38235856]\n",
      " [ 0.64542744  0.35457256]\n",
      " [ 0.84177101  0.15822899]\n",
      " [ 0.68708046  0.31291954]\n",
      " [ 0.78642007  0.21357993]\n",
      " [ 0.61640058  0.38359942]\n",
      " [ 0.8077402   0.1922598 ]\n",
      " [ 0.67809401  0.32190599]\n",
      " [ 0.51699954  0.48300046]\n",
      " [ 0.67828773  0.32171227]\n",
      " [ 0.40500438  0.59499562]\n",
      " [ 0.84118647  0.15881353]\n",
      " [ 0.39833601  0.60166399]\n",
      " [ 0.57314343  0.42685657]\n",
      " [ 0.45253211  0.54746789]\n",
      " [ 0.79017424  0.20982576]\n",
      " [ 0.71288767  0.28711233]\n",
      " [ 0.60682612  0.39317388]\n",
      " [ 0.70425521  0.29574479]\n",
      " [ 0.77980782  0.22019218]\n",
      " [ 0.62875158  0.37124842]\n",
      " [ 0.80693874  0.19306126]\n",
      " [ 0.54108104  0.45891896]\n",
      " [ 0.78065962  0.21934038]\n",
      " [ 0.46812832  0.53187168]\n",
      " [ 0.84405439  0.15594561]\n",
      " [ 0.78938329  0.21061671]\n",
      " [ 0.55886776  0.44113224]\n",
      " [ 0.77085606  0.22914394]\n",
      " [ 0.73339454  0.26660546]\n",
      " [ 0.61370392  0.38629608]\n",
      " [ 0.77234735  0.22765265]\n",
      " [ 0.56806199  0.43193801]\n",
      " [ 0.58126526  0.41873474]\n",
      " [ 0.83325627  0.16674373]\n",
      " [ 0.72735773  0.27264227]\n",
      " [ 0.82003641  0.17996359]\n",
      " [ 0.84459068  0.15540932]\n",
      " [ 0.54940802  0.45059198]\n",
      " [ 0.73225178  0.26774822]\n",
      " [ 0.68939867  0.31060133]\n",
      " [ 0.75609062  0.24390938]\n",
      " [ 0.60585225  0.39414775]\n",
      " [ 0.75898746  0.24101254]\n",
      " [ 0.60877574  0.39122426]\n",
      " [ 0.83487184  0.16512816]\n",
      " [ 0.68544561  0.31455439]\n",
      " [ 0.81251712  0.18748288]\n",
      " [ 0.60821274  0.39178726]\n",
      " [ 0.80999746  0.19000254]\n",
      " [ 0.56741766  0.43258234]\n",
      " [ 0.80947424  0.19052576]\n",
      " [ 0.78939655  0.21060345]\n",
      " [ 0.73563475  0.26436525]\n",
      " [ 0.47511791  0.52488209]\n",
      " [ 0.78334828  0.21665172]\n",
      " [ 0.78759474  0.21240526]\n",
      " [ 0.58765076  0.41234924]\n",
      " [ 0.84600354  0.15399646]\n",
      " [ 0.61051529  0.38948471]\n",
      " [ 0.71472698  0.28527302]\n",
      " [ 0.72158547  0.27841453]\n",
      " [ 0.79756639  0.20243361]\n",
      " [ 0.62956691  0.37043309]\n",
      " [ 0.73839072  0.26160928]\n",
      " [ 0.72683676  0.27316324]\n",
      " [ 0.70103225  0.29896775]\n",
      " [ 0.63804077  0.36195923]\n",
      " [ 0.477304    0.522696  ]\n",
      " [ 0.79453048  0.20546952]\n",
      " [ 0.68025885  0.31974115]\n",
      " [ 0.41922616  0.58077384]\n",
      " [ 0.6585493   0.3414507 ]\n",
      " [ 0.85264457  0.14735543]\n",
      " [ 0.5060552   0.4939448 ]\n",
      " [ 0.84853572  0.15146428]\n",
      " [ 0.7925686   0.2074314 ]\n",
      " [ 0.72293435  0.27706565]\n",
      " [ 0.69051254  0.30948746]\n",
      " [ 0.46035803  0.53964197]\n",
      " [ 0.63038584  0.36961416]\n",
      " [ 0.60109742  0.39890258]\n",
      " [ 0.81067096  0.18932904]\n",
      " [ 0.49787683  0.50212317]\n",
      " [ 0.57142814  0.42857186]\n",
      " [ 0.43329584  0.56670416]\n",
      " [ 0.58474593  0.41525407]\n",
      " [ 0.80978867  0.19021133]\n",
      " [ 0.78349572  0.21650428]\n",
      " [ 0.77378218  0.22621782]\n",
      " [ 0.66162614  0.33837386]\n",
      " [ 0.65756325  0.34243675]\n",
      " [ 0.76862981  0.23137019]\n",
      " [ 0.6159137   0.3840863 ]\n",
      " [ 0.74187465  0.25812535]\n",
      " [ 0.513499    0.486501  ]\n",
      " [ 0.80570577  0.19429423]\n",
      " [ 0.71466919  0.28533081]\n",
      " [ 0.72474293  0.27525707]\n",
      " [ 0.71299849  0.28700151]\n",
      " [ 0.83740591  0.16259409]\n",
      " [ 0.83330931  0.16669069]\n",
      " [ 0.65492573  0.34507427]\n",
      " [ 0.67355033  0.32644967]\n",
      " [ 0.76207314  0.23792686]\n",
      " [ 0.78625755  0.21374245]\n",
      " [ 0.79611462  0.20388538]\n",
      " [ 0.56181797  0.43818203]\n",
      " [ 0.53243954  0.46756046]\n",
      " [ 0.59604602  0.40395398]\n",
      " [ 0.82055154  0.17944846]\n",
      " [ 0.60918897  0.39081103]\n",
      " [ 0.58425049  0.41574951]\n",
      " [ 0.79930517  0.20069483]\n",
      " [ 0.8519198   0.1480802 ]\n",
      " [ 0.66456302  0.33543698]]\n"
     ]
    }
   ],
   "source": [
    "# predict class probabilities\n",
    "\n",
    "probability = model3.predict_proba(X2_test)\n",
    "print probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y2_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78  4]\n",
      " [32  6]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.95      0.81        82\n",
      "          1       0.60      0.16      0.25        38\n",
      "\n",
      "avg / total       0.67      0.70      0.63       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.confusion_matrix(y2_test, predicted)\n",
    "print metrics.classification_report(y2_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71604938  0.72839506  0.7125      0.6835443   0.69620253]\n",
      "0.707338255977\n"
     ]
    }
   ],
   "source": [
    "scores3 = cross_val_score(linear_model.LogisticRegression(), X2, Y2, scoring='accuracy', cv=5)\n",
    "print scores3\n",
    "print scores3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, our score comes out at around 70 per cent for the model (save a dip in the case of two folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't show a huge difference compared to the last round in terms of score, accuracy, recall and precision.  We may be able to more clearly see the effect that rank is having in the data, but we still have potential variance bias introduced by gre.   Were we perhaps to exclude gre from the model, we might perhaps improve its accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
